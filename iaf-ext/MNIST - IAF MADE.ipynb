{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is strongly inspired by the Keras VAE tutorial from fchollet (https://github.com/fchollet/keras/blob/master/examples/variational_autoencoder.py).\n",
    "\n",
    "It implements IAF from DP Kingma et al. (https://arxiv.org/abs/1606.04934) with MADE autoregressive models from Germain et al. (https://arxiv.org/pdf/1502.03509.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from keras.layers import Input, Dense, Lambda, Add, Multiply, PReLU\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import math\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "original_dim = 784\n",
    "latent_dim = 2\n",
    "intermediate_dim = 256\n",
    "epochs = 50\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean_0 = Dense(latent_dim)(h)\n",
    "z_std_0 = Dense(latent_dim, activation=\"softplus\")(h)\n",
    "hp = Dense(latent_dim)(h)\n",
    "encoder = Model(x, [z_mean_0,z_std_0, hp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Latent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MaskedDense(Dense):\n",
    "    \"\"\"A dense layer with a masking possibilities\"\"\"\n",
    "\n",
    "    def __init__(self, units, mask, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                 kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None,\n",
    "                 transpose=False, **kwargs):\n",
    "        super(MaskedDense, self).__init__(units, bias_initializer=bias_initializer,\n",
    "                                          activation=activation, kernel_initializer=kernel_initializer,\n",
    "                                          kernel_regularizer=kernel_regularizer, bias_regularizer=bias_regularizer, activity_regularizer=activity_regularizer,\n",
    "                                          kernel_constraint=kernel_constraint, bias_constraint=bias_constraint,\n",
    "                                          use_bias=use_bias, **kwargs)\n",
    "        if not transpose:\n",
    "            self.mask = K.variable(mask)\n",
    "        else:\n",
    "            self.mask = K.variable(mask.T)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = K.dot(x, Multiply()([self.kernel, self.mask]))\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias)\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "def _mask_matrix_made(dim):\n",
    "    \"\"\"A generator of masks for two-layered MADE model (see https://arxiv.org/pdf/1502.03509.pdf)\"\"\"\n",
    "    mask_vector = np.random.randint(1, dim, dim)\n",
    "    mask_matrix0 = np.fromfunction(lambda k, d: mask_vector[k] >= d, (dim, dim), dtype=int).astype(np.int32).astype(np.float32)\n",
    "    mask_matrix1 = np.fromfunction(lambda d, k: d > mask_vector[k], (dim, dim), dtype=int).astype(np.int32).astype(np.float32)\n",
    "    return mask_matrix0, mask_matrix1\n",
    "\n",
    "\n",
    "def MADE(mask_matrix0, mask_matrix1, latent_dim):\n",
    "    \"\"\"A 2-layered MADE model (https://arxiv.org/pdf/1502.03509.pdf)\"\"\"\n",
    "    def f(x):\n",
    "        hl = MaskedDense(latent_dim, mask=mask_matrix0)(x)\n",
    "        hl = PReLU()(hl)\n",
    "        std = MaskedDense(latent_dim, mask=mask_matrix1, activation=\"softplus\")(hl)\n",
    "        mean = MaskedDense(latent_dim, mask=mask_matrix1, activation=None)(hl)\n",
    "        return mean, std\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_latent = 10  # the number of IAF transform you want to apply\n",
    "latent_models = []\n",
    "\n",
    "masks = [_mask_matrix_made(latent_dim) for k in range(n_latent)]\n",
    "    \n",
    "for k in range(n_latent):\n",
    "    latent_input = Input(shape=(latent_dim,), batch_shape=(batch_size, latent_dim))\n",
    "\n",
    "    mask0, mask1 = masks[k]\n",
    "    mean, std = MADE(mask0, mask1, latent_dim)(latent_input)\n",
    "\n",
    "    latent_model = Model(latent_input, [mean, std])\n",
    "    latent_models.append(latent_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_eps(batch_size, latent_dim, epsilon_std):\n",
    "    \"\"\"Create a function to sample N(0, epsilon_std) vectors\"\"\"\n",
    "    return lambda args: K.random_normal(shape=(batch_size, latent_dim),\n",
    "                                        mean=0.,\n",
    "                                        stddev=epsilon_std)\n",
    "\n",
    "def sample_z0(args):\n",
    "    \"\"\"Sample from N(mu, sigma) where sigma is the stddev !!!\"\"\"\n",
    "    z_mean, z_std, epsilon = args\n",
    "    z0 = z_mean + K.exp(K.log(z_std + 1e-8)) * epsilon\n",
    "    return z0\n",
    "\n",
    "def iaf_transform_z(args):\n",
    "    \"\"\"Apply the IAF transform to input z (https://arxiv.org/abs/1606.04934)\"\"\"\n",
    "    z, mean, std = args\n",
    "    z_ = z\n",
    "    z_ -= mean\n",
    "    z_ /= std\n",
    "    return z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eps = Lambda(sample_eps(batch_size, latent_dim, epsilon_std), name='sample_eps')([z_mean_0, z_std_0])\n",
    "z0 = Lambda(sample_z0, name='sample_z0')([z_mean_0, z_std_0, eps])\n",
    "\n",
    "z_means = [z_mean_0]\n",
    "z_stds = [z_std_0]\n",
    "zs = [z0]\n",
    "for latent_model in latent_models:\n",
    "    zz = Add()([Dense(latent_dim, activation='relu')(hp), zs[-1]])\n",
    "    z_mean, z_std = latent_model(zz)\n",
    "    z_means.append(z_mean)\n",
    "    z_stds.append(z_std)\n",
    "    z = Lambda(iaf_transform_z)([zs[-1], z_mean, z_std])\n",
    "    zs.append(z)\n",
    "z = zs[-1]\n",
    "\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def log_stdnormal(x):\n",
    "    \"\"\"log density of a standard gaussian\"\"\"\n",
    "    c = - 0.5 * math.log(2*math.pi)\n",
    "    result = c - K.square(x) / 2\n",
    "    return result\n",
    "\n",
    "def log_normal2(x, mean, log_var):\n",
    "    \"\"\"log density of N(mu, sigma)\"\"\"\n",
    "    c = - 0.5 * math.log(2*math.pi)\n",
    "    result = c - log_var/2 - K.square(x - mean) / (2 * K.exp(log_var) + 1e-8)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_sample = 20\n",
    "\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    \"\"\"\n",
    "    Variationnal lower bound\n",
    "    This is the cross entropy minus the KL(Q(.|z)||P(.))\n",
    "    The latter term is estimated by Monte Carlo sampling\n",
    "    \"\"\"\n",
    "    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "\n",
    "    # kl divergence\n",
    "    # sampling for estimating the expectations\n",
    "    for k in range(n_sample):\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                                  stddev=1.0)  # used for every z_i sampling\n",
    "        z0_ = z_mean_0 + z_std_0 * epsilon\n",
    "        z_ = z0_\n",
    "        for z_mean, z_std in zip(z_means[1:], z_stds[1:]):\n",
    "            z_ = iaf_transform_z([z_, z_mean, z_std])\n",
    "\n",
    "        try:\n",
    "            loss += K.sum(log_normal2(z0_, z_mean_0, 2 * K.log(z_std_0 + 1e-8)), -1)\n",
    "        except NameError:\n",
    "            loss = K.sum(log_normal2(z0_, z_mean_0, 2 * K.log(z_std_0 + 1e-8)), -1)\n",
    "        loss -= K.sum(log_stdnormal(z_), -1)\n",
    "    # don't forget the log_std_sum.\n",
    "    # BE CAUTIOUS!! THE LOG_STD_SUM_0 HAS ALREADY BEEN TAKEN INTO ACCOUNT IN LOSS\n",
    "    kl_loss = loss / n_sample\n",
    "    for z_std in z_stds[1:]:\n",
    "        kl_loss += K.sum(K.log(1e-8 + z_std), -1)\n",
    "\n",
    "    return xent_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae = Model(x, x_decoded_mean)\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train the VAE on MNIST digits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tb = TensorBoard(log_dir='./log/MNIST/IAF_10_layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faddd3cfdd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        callbacks=[tb],\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Play with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "n = 15  # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = generator.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}